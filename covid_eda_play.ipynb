{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use below command in pip if widget extensions aren't displaying\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from io import StringIO\n",
    "from urllib.request import urlopen\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "from statsmodels.robust.scale import mad\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retrieving data from NY Times GIT...'\\\n",
    "     'Note that the data is usually two days behind.')\n",
    "url='https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'\n",
    "resp=requests.get(url).text\n",
    "df=pd.read_csv(StringIO(resp))\n",
    "\n",
    "#need to add save to file in case the service goes down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uploading State CSV file...')\n",
    "df_csv=pd.read_csv('state_locs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many rows are in the table\n",
    "rowcount = df.shape[0]\n",
    "\n",
    "# determine how many rows have null FIPS \n",
    "null_fips_rowcount = df.loc[df['fips'].isnull()].shape[0]\n",
    "\n",
    "# calculate how much of the data this represents as a percentage\n",
    "percentage_null_fips = round((null_fips_rowcount / rowcount) * 100, 2)\n",
    "\n",
    "print(\"There were \"+str(null_fips_rowcount)+\" records with null FIPS values in the data.\\nThis amounts to \" +str(percentage_null_fips)+\"% of the available data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe without null FIPS records\n",
    "df = df.loc[df['fips'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# convert FIPS data to int to remove decimal point\n",
    "df = df.astype({'fips': 'int64'})\n",
    "# convert back to str for processing\n",
    "df = df.astype({'fips': 'str'})\n",
    "\n",
    "# check how many records have a FIPS value with four characters\n",
    "trunc_df = df.loc[df['fips'].str.len() == 4]\n",
    "trunc_data_per = (trunc_df.shape[0] / df.shape[0])*100\n",
    "\n",
    "# use another print statement (using the f format key) to report this information\n",
    "print(f\"{round(trunc_data_per, 2)}% of data ({trunc_df.shape[0]} rows) has truncated FIPS values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad zero to fips with len 4\n",
    "conditions = [df['fips'].str.len()==4]\n",
    "choices = ['0'+ df['fips']]\n",
    "\n",
    "df['fips'] = np.select(conditions, choices, default=df['fips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how far back to limit the data\n",
    "DAYS_BACK= 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_limit(df, days_back=14) -> pd.DataFrame():\n",
    "    \"\"\"\n",
    "    Common date filter for dfs\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    today = pd.to_datetime('today')\n",
    "    df.loc[:,'days_since'] = (today-df.index).days\n",
    "    \n",
    "    return df.loc[df['days_since']<=days_back]\n",
    "    \n",
    "\n",
    "def determine_daily_counts(group) -> pd.DataFrame():\n",
    "    \"\"\"\n",
    "    Calculate daily cases/death counts\n",
    "    cases are cumulative, so using the diff method \n",
    "    to determine daily counts\n",
    "    \"\"\"    \n",
    "    group.loc[:,'daily_case_count'] = group['cases']\n",
    "    group.loc[:,'daily_case_count'] = group['cases'].diff().fillna(group['cases']).astype('int32')\n",
    "    group.loc[:,'daily_death_count'] = group['deaths'].diff().fillna(0).astype('int32')\n",
    "    return group\n",
    "\n",
    "\n",
    "def determine_est_of_location(df, col):\n",
    "    \"\"\"\n",
    "    Prints common estimate of location \n",
    "    information for data\n",
    "    \"\"\"\n",
    "    print(f'\\nMax: {df[col].describe()[7]}')\n",
    "    print(f'Min: {df[col].describe()[3]}')\n",
    "    print(f'Mean: {round(df[col].describe()[1],2)}')\n",
    "    print(f'Trimmed Mean: {round(stats.trim_mean(df[col], proportiontocut=0.1),2)}')\n",
    "    print(f'Median: {df[col].median()}')\n",
    "\n",
    "\n",
    "def determine_skewness(df, col):\n",
    "    \"\"\"\n",
    "    Determines skewness of data\n",
    "    \"\"\"\n",
    "    skew_val = skew(df[col])\n",
    "    kurtosis_val = kurtosis(df[col])\n",
    "    \n",
    "    print(f'skewness value of: {round(skew_val,2)}')\n",
    "    # if the index is between -1 and 1, then the distribution is symmetric. \n",
    "    if -1 <= skew_val <= 1:\n",
    "        print('data is symmetric')\n",
    "    # if the index is no more than -1 then it is skewed to the left \n",
    "    if skew_val <= -1:\n",
    "        print('data is left skewed')\n",
    "        \n",
    "    # if it is at least 1, then it is skewed to the right\n",
    "    if skew_val >=1:\n",
    "        print('data is right skewed')\n",
    "    \n",
    "    print(f'\\nkurtosis value of: {round(kurtosis_val,2)}')\n",
    "    # Leptokurtic (Kurtosis > 3): Distribution is longer, tails are fatter. data are heavy-tailed or profusion of outliers\n",
    "    if kurtosis_val > 3:\n",
    "        print('distribution is longer and data are heavy-tailed/profusion of outliers')\n",
    "    \n",
    "    # Platykurtic: (Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. \n",
    "    if kurtosis_val < 3:\n",
    "        print('distribution is shorter, tails are thinner than normal distribution')\n",
    "    # data are light-tailed or lack of outliers.\n",
    "\n",
    "\n",
    "def determine_est_of_variability(df, col):\n",
    "    \"\"\"\n",
    "    Prints common estimate of variability \n",
    "    information for data\n",
    "    \"\"\"\n",
    "    print(f'\\nSTD: {round(np.std(df[col]),2)}') # population STD\n",
    "    temp = df.describe(include=[np.number],percentiles=[.10,.90]).T\n",
    "    tstd = stats.tstd(df[col],(temp['10%'].tolist()[0],temp['90%'].tolist()[0]))\n",
    "    print(f'Trimmed STD: {round(tstd,2)}')\n",
    "    print(f'IQR: {df[col].quantile(0.75) - df[col].quantile(0.25)}')\n",
    "    print(f'Mean absolute deviation: {round(df[col].mad(),2)}')\n",
    "    print(f'Median absolute deviation: {round(mad(df[col]),2)}')\n",
    "    \n",
    "\n",
    "def fix_negative_values(df, col1, col2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Finds negative values and \n",
    "    sets to calculated value\n",
    "    \"\"\"\n",
    "    condictions = [df[col1] < 0]\n",
    "    choices = [df[col2]]\n",
    "    return np.select(condictions, choices, default=df[col1]).astype('int32')\n",
    "\n",
    "\n",
    "def graph_data(df, column, chart_type, chart_name='data graphed', chart_title='title',color='rgb(26, 118, 255)'):\n",
    "    \"\"\"\n",
    "    Main graph function\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # caluculate rolling mean for every 4 days. This is not cumlative.\n",
    "    df.loc[:,'rolling_mean'] = df[column].rolling(window=4).mean()\n",
    "    df.loc[:,'rolling_mean'].fillna(0, inplace=True)\n",
    "    \n",
    "    # only going to display past x days\n",
    "    df = date_limit(df,days_back=DAYS_BACK)\n",
    "    \n",
    "    if chart_type == 'line':\n",
    "        # graph daily cases and mean as line chart\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df[column],\n",
    "                        mode='lines+markers',\n",
    "                        line=dict(color=f'{color}', width=2),\n",
    "                        name=f'{chart_name}'))\n",
    "        fig.update_layout(title=chart_title)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df['rolling_mean'], name='Rolling Mean',\n",
    "                                 line = dict(color=f'{color}', width=2, dash='dash')))\n",
    "\n",
    "        fig.update_xaxes(rangeslider_visible=True, title='Date Slider')\n",
    "        fig.show()\n",
    "    \n",
    "    elif chart_type == 'bar':\n",
    "        # graph weekday daily cases count as bar graph\n",
    "        df['dtg'] = pd.to_datetime(df['date'])\n",
    "        weekday = df['dtg'].dt.day_name()\n",
    "        weekday_daily_sum = df.groupby(weekday)[column].sum().to_frame()\n",
    "        \n",
    "        # bar chart name\n",
    "        state_name = df.state.unique()[0]\n",
    "        \n",
    "        # sort weekday df\n",
    "        sorter = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "        sorterIndex = dict(zip(sorter,range(len(sorter))))\n",
    "        weekday_daily_sum['day_id'] = weekday_daily_sum.index\n",
    "        weekday_daily_sum['day_id'] = weekday_daily_sum['day_id'].map(sorterIndex)\n",
    "        weekday_daily_sum.sort_values('day_id', inplace=True)\n",
    "        \n",
    "        # generate weekd day bar graph\n",
    "        fig = go.Figure([go.Bar(x=weekday_daily_sum.index, y=weekday_daily_sum[column],\n",
    "                        text=weekday_daily_sum[column],textposition='auto',marker_color=color)])\n",
    "        fig.update_layout(title=f'{state_name}')\n",
    "        fig.show()\n",
    "    \n",
    "    elif chart_type == 'violin':\n",
    "        fig = px.violin(df, y=column, points='all', height=600)\n",
    "        fig.show()\n",
    "        \n",
    "    elif chart_type == 'histogram':\n",
    "        fig = px.histogram(df, x='daily_case_count', histnorm='percent', marginal='rug')\n",
    "        fig.show()\n",
    "\n",
    "        \n",
    "ALL = 'ALL'\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def get_county_geom(df, counties) -> dict:\n",
    "    \"\"\"\n",
    "    Returns county geoms for selected\n",
    "    state in df\n",
    "    \"\"\"\n",
    "    \n",
    "    fips_list = df.fips.unique()\n",
    "    county_geoms = {\"type\": \"FeatureCollection\", \"features\":[]}\n",
    "    \n",
    "    for rec in counties['features']:\n",
    "        if rec['id'] in fips_list:\n",
    "            county_geoms['features'].append(rec)\n",
    "    \n",
    "    return county_geoms\n",
    "\n",
    "\n",
    "def get_county_lat_lon(geoms) -> float:\n",
    "    \"\"\"\n",
    "    Returns mean lon, lat coords \n",
    "    of geoms \n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    lons, lats = [], []\n",
    "    for rec in geoms['features']:\n",
    "        try:\n",
    "            lons.append(np.mean(np.asarray(rec['geometry']['coordinates'],dtype=object), axis=(0,1))[0])\n",
    "            lats.append(np.mean(np.asarray(rec['geometry']['coordinates'],dtype=object), axis=(0,1))[1])\n",
    "        except(IndexError,TypeError) as err:\n",
    "            continue\n",
    "    return round(np.mean(lons),2), round(np.mean(lats),2)\n",
    "\n",
    "\n",
    "def map_data(df, size_column, title, map_type, lat='lat', lon='lon'):\n",
    "    \"\"\"\n",
    "    Main mapping function\n",
    "    \"\"\"\n",
    "    if map_type == 'scatter':\n",
    "        df['dtg'] = df.index.astype(str)\n",
    "        fig = px.scatter_geo(df, locationmode = 'USA-states', lat=lat, lon=lon,\n",
    "                         hover_name=\"state\", size=size_column, animation_frame=\"dtg\",)\n",
    "        fig.update_layout(\n",
    "            title_text = title,\n",
    "            showlegend = True,\n",
    "            geo = dict(\n",
    "                scope = 'usa',\n",
    "                landcolor = 'rgb(217, 217, 217)',))\n",
    "        fig.show()\n",
    "    elif map_type == 'choropleth':\n",
    "        # get counties geom\n",
    "        with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "            counties = json.load(response)\n",
    "            \n",
    "            with open(\"counties_geom.json\", \"w\") as f:\n",
    "                json.dump(counties,f,indent=4,sort_keys=True)\n",
    "            \n",
    "            # filter counties dict to state specific per df\n",
    "            state_counties = get_county_geom(df, counties)\n",
    "            \n",
    "            # get mean lon/lat of counties geom\n",
    "            lon, lat = get_county_lat_lon(state_counties)\n",
    "        \n",
    "        # blues\n",
    "        fig = px.choropleth_mapbox(df, geojson=state_counties, locations=df['fips'], color=size_column,\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=5, center = {\"lat\": lat, \"lon\": lon},hover_data=['county', 'total_cnt'],\n",
    "                           opacity=0.5,\n",
    "                           labels={'county':'County', 'daily_case_count':'Cases'},\n",
    "#                            animation_frame = 'date',\n",
    "                                   title=title\n",
    "                          )\n",
    "        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "        fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create states only no county df\n",
    "states_df = df.groupby(['state', 'date']).sum().reset_index()\n",
    "states_df.loc[:,'dtg'] = states_df['date']\n",
    "states_df.set_index(pd.DatetimeIndex(states_df['date']), inplace=True)\n",
    "states_df = states_df[['state', 'cases', 'deaths', 'date']]\n",
    "\n",
    "# create group obj to perform specific state operations\n",
    "states_df2 = states_df.groupby('state').apply(determine_daily_counts)\n",
    "\n",
    "# find negative daily counts\n",
    "states_df2.loc[:,'negative_cases_daily'] = states_df2['daily_case_count'].apply(lambda x: x < 0)\n",
    "states_df2.loc[:,'negative_deaths_daily'] = states_df2['daily_death_count'].apply(lambda x: x < 0)\n",
    "\n",
    "# check how many records\n",
    "negative_cnts_df = states_df2.loc[(states_df2['negative_cases_daily']== True) | (states_df2['negative_deaths_daily']== True)]\n",
    "negative_cnts_per = (negative_cnts_df.shape[0] / states_df2.shape[0])*100\n",
    "print(f'{round(negative_cnts_per, 2)}% of the data ({negative_cnts_df.shape[0]} rows out of {states_df2.shape[0]}) have negative counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see records with bad values\n",
    "# states_df2.loc[(states_df2['negative_cases_daily']== True) | (states_df2['negative_deaths_daily']== True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find records with negative count and use previous day count\n",
    "# think about infering back records instead of setting to day prior \n",
    "# add count in new column\n",
    "states_df2.loc[:,'daily_case_count2'] = states_df2['daily_case_count'].shift(1).where(states_df2['daily_case_count'] <0)\n",
    "states_df2.loc[:,'daily_deaths_count2'] = states_df2['daily_death_count'].shift(1).where(states_df2['daily_death_count'] <0)\n",
    "\n",
    "# find negative counts and replace with previous value\n",
    "states_df2.loc[:,'daily_case_count'] = fix_negative_values(states_df2,'daily_case_count', 'daily_case_count2')\n",
    "states_df2.loc[:,'daily_death_count'] = fix_negative_values(states_df2,'daily_death_count', 'daily_deaths_count2')\n",
    "\n",
    "# # add locations to states\n",
    "states_df2 = pd.merge(states_df2, df_csv, left_on='state', right_on='state_name')\n",
    "\n",
    "# format to wanted columns\n",
    "states_df2 = states_df2[['state', 'cases', 'deaths', 'date', 'daily_case_count', 'daily_death_count', 'lat', 'lon']]\n",
    "states_df2.set_index(pd.DatetimeIndex(states_df2['date']), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_df2.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tidy Data DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_cases_df = states_df2[['state', 'daily_case_count','lat', 'lon']]\n",
    "states_deaths_df = states_df2[['state', 'daily_death_count','lat', 'lon']]\n",
    "\n",
    "# time limit df\n",
    "state_cases_past_xdays = date_limit(states_cases_df,days_back=DAYS_BACK)\n",
    "\n",
    "# create sample dfs 10% of the data\n",
    "sample_states_cases = states_cases_df.sample(frac =.10)\n",
    "sample_states_deaths = states_deaths_df.sample(frac =.10)\n",
    "sample_state_cases_past_xdays = state_cases_past_xdays.sample(frac =.10)\n",
    "\n",
    "# dates list\n",
    "dates_list = sorted(list(set(states_df2.loc[:,'date'].tolist())))\n",
    "\n",
    "# create most recent date states df\n",
    "most_recent_date_states_df = states_df2.groupby('state').tail(1)\n",
    "most_recent_date_cases_df = most_recent_date_states_df[['state','daily_case_count']]\n",
    "most_recent_date_deaths_df = most_recent_date_states_df[['state','daily_death_count']]\n",
    "\n",
    "# create past x days cases sum df\n",
    "state_cases_past_xdays_sum = state_cases_past_xdays.groupby('state')['daily_case_count'].agg('sum').to_frame().reset_index()\n",
    "state_cases_past_xdays_sum.rename(columns = {'daily_case_count':'total_cnt'}, inplace = True)\n",
    "\n",
    "# create dates count df\n",
    "s = states_df2.groupby(states_df2.index)['daily_case_count'].sum()\n",
    "date_cnts_df = s.to_frame().reset_index()\n",
    "\n",
    "# need to sort df by state, county, & date to calculate county cases\n",
    "st_cnty_sorted = df.sort_values(['state','county','date'])\n",
    "\n",
    "st_cnty_df = st_cnty_sorted.groupby(['state','county']).apply(determine_daily_counts)\n",
    "st_cnty_df.set_index(pd.DatetimeIndex(st_cnty_df['date']), inplace=True)\n",
    "st_cnty_df_past_xdays = date_limit(st_cnty_df,days_back=DAYS_BACK)\n",
    "\n",
    "# create state, county df's\n",
    "st_cnty_cases_df = st_cnty_df[['state', 'county','fips','daily_case_count']]\n",
    "st_cnty_cases_df_past_xdays = st_cnty_df_past_xdays[['state', 'county','fips','daily_case_count']]\n",
    "st_cnty_deaths_df = st_cnty_df[['state', 'county','fips','daily_death_count']]\n",
    "\n",
    "# time limit states with county df\n",
    "st_cnty_cases_past_xdays = date_limit(st_cnty_cases_df,days_back=DAYS_BACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_cases_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_states_cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cases_past_xdays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state_cases_past_xdays.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_cases_past_xdays.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate location of data\n",
    "determine_est_of_location(sample_state_cases_past_xdays, 'daily_case_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate variability of data\n",
    "determine_est_of_variability(sample_state_cases_past_xdays, 'daily_case_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whether the data is symmetric or skewed\n",
    "determine_skewness(sample_state_cases_past_xdays, 'daily_case_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_state_cases_past_xdays['daily_case_count'].quantile([0.05,.25,.5,.75,.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hist graphing distribution\n",
    "# ax = sample_state_cases_past_xdays.hist(column=['daily_case_count'], bins='auto', alpha=0.5, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(sample_state_cases_past_xdays, x='daily_case_count', histnorm='percent', marginal='rug', nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(sample_state_cases_past_xdays, y='daily_case_count', points='all', height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe Most Recent Day Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Date: {most_recent_date_cases_df.index.tolist()[0].strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "determine_est_of_location(most_recent_date_cases_df, 'daily_case_count')\n",
    "determine_est_of_variability(most_recent_date_cases_df, 'daily_case_count')\n",
    "\n",
    "# determine percentage increase/decrease of current cases compared to previous day\n",
    "previous_day_cnt = states_df2.loc[states_df2.index == dates_list[-2]]['daily_case_count'].sum()\n",
    "total_daily_case_count = most_recent_date_states_df['daily_case_count'].sum()\n",
    "most_rc_date = most_recent_date_states_df['date'].tolist()[0]\n",
    "\n",
    "# check to see if new total is less than previous day\n",
    "# if so determine difference\n",
    "if total_daily_case_count < previous_day_cnt:\n",
    "    decrease = previous_day_cnt - total_daily_case_count\n",
    "    percnt_decrease = round(decrease / previous_day_cnt * 100, 2)\n",
    "    print(f'\\n{most_rc_date} had {total_daily_case_count} total cases, ' \n",
    "          f'with a decrease of {decrease} or {percnt_decrease}% compared to previous day')\n",
    "else:\n",
    "    increase = total_daily_case_count - previous_day_cnt\n",
    "    percnt_increase = round(increase / previous_day_cnt * 100,2)\n",
    "    print(f'\\n{most_rc_date} had {total_daily_case_count} total cases, ' \n",
    "          f'with an increase of {increase} or {percnt_increase}% compared to previous day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 states with most cases for most recent\n",
    "most_recent_date_cases_df.nlargest(10,'daily_case_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 states with most cases for past x days\n",
    "state_cases_past_xdays_sum.nlargest(10,'total_cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cnts_df.set_index(pd.DatetimeIndex(date_cnts_df['date']), inplace=True)\n",
    "# plot US daily totals\n",
    "graph_data(date_cnts_df, 'daily_case_count', 'line', chart_name='Daily Cases', chart_title=f'US {DAYS_BACK} Day Totals',color='royalblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data(state_cases_past_xdays, 'daily_case_count', 'US Daily Cases By Date', 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_state = widgets.Dropdown(options = unique_sorted_values_plus_ALL(states_df2.state), description='State: ')\n",
    "\n",
    "output_state = widgets.Output()\n",
    "cases_plot_output = widgets.Output()\n",
    "deaths_plot_output = widgets.Output()\n",
    "weekday_plot = widgets.Output()\n",
    "distro_plot = widgets.Output()\n",
    "density_plot = widgets.Output()\n",
    "\n",
    "def event_action():\n",
    "    # clear the previous selection on each iteration\n",
    "    output_state.clear_output()\n",
    "    cases_plot_output.clear_output()\n",
    "    deaths_plot_output.clear_output()\n",
    "    weekday_plot.clear_output()\n",
    "    distro_plot.clear_output()\n",
    "    density_plot.clear_output()\n",
    "    \n",
    "    if (dropdown_state.value == ALL):\n",
    "        common_filter = states_df2\n",
    "        common_map_filter = st_cnty_df\n",
    "    else:\n",
    "        common_filter = states_df2[states_df2.state == dropdown_state.value]\n",
    "        common_map_filter = st_cnty_df_past_xdays[st_cnty_df_past_xdays.state == dropdown_state.value]\n",
    "    \n",
    "    with output_state:\n",
    "        display(common_filter.last(pd.offsets.Day(DAYS_BACK)))\n",
    "    \n",
    "    with cases_plot_output:\n",
    "        graph_data(common_filter, 'daily_case_count', 'line', chart_name='Daily Cases', chart_title=common_filter.state[0],color='royalblue')\n",
    "        \n",
    "    with deaths_plot_output:\n",
    "        graph_data(common_filter, 'daily_death_count', 'line',chart_name='Daily Cases', chart_title=common_filter.state[0], color='firebrick')\n",
    "    \n",
    "    with weekday_plot:\n",
    "        graph_data(common_filter, 'daily_case_count', 'bar',)\n",
    "    \n",
    "    with distro_plot:\n",
    "        graph_data(common_filter, 'daily_case_count', 'histogram')\n",
    "    \n",
    "    with density_plot:\n",
    "        \n",
    "        # group data by county and get daily case sum\n",
    "        common_map_filter = common_map_filter.groupby(['county', 'fips'], as_index=False)['daily_case_count'].agg('sum')\n",
    "        common_map_filter.rename(columns = {'daily_case_count':'total_cnt'}, inplace = True)\n",
    "        print(f'Top 10 counties with most cases in past {DAYS_BACK} days:\\n {common_map_filter.nlargest(10,\"total_cnt\")}')\n",
    "        \n",
    "        map_data(common_map_filter,'total_cnt', 'State Cases Density', 'choropleth')\n",
    "\n",
    "def dropdown_state_eventhandler(change):\n",
    "    event_action()\n",
    "    \n",
    "def graphit():\n",
    "    event_action()\n",
    "    \n",
    "def tab_chg(chg):\n",
    "    if chg.old == {}:\n",
    "        graphit()\n",
    "\n",
    "dropdown_state.observe(dropdown_state_eventhandler, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_widgets = widgets.HBox([dropdown_state])\n",
    "\n",
    "tab = widgets.Tab([output_state, distro_plot, cases_plot_output, deaths_plot_output, weekday_plot, density_plot])\n",
    "tab.set_title(0, 'Dataset')\n",
    "tab.set_title(1, 'Distribution Graph')\n",
    "tab.set_title(2, 'Daily Cases Graph')\n",
    "tab.set_title(3, 'Daily Deaths Graph')\n",
    "tab.set_title(4, 'Weekday Counts')\n",
    "tab.set_title(5, 'State Density Plot')\n",
    "tab.observe(tab_chg)\n",
    "\n",
    "dashboard = widgets.VBox([tab], layout=Layout(height='700px'))\n",
    "display(input_widgets, dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data used for filter operations\n",
    "df3 = st_cnty_df\n",
    "\n",
    "# create initial drop down options\n",
    "# countyW = widgets.Dropdown(options = df3.county.unique(), description='County: ')\n",
    "\n",
    "def filter_and_graph(**func_kwargs):\n",
    "    \"\"\"\n",
    "    filters df and calls graph function\n",
    "    based on selected operations\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert user params to list\n",
    "    l = list(func_kwargs.values())\n",
    "    # filter df based on updated values\n",
    "    df_updated = df3.query(f'state==\"{l[0]}\" and county==\"{l[1]}\"')\n",
    "    \n",
    "    # call function to graph data\n",
    "    graph_data(df_updated, 'daily_case_count', 'line', chart_name='Daily Cases', chart_title=l[1],color='royalblue')\n",
    "    \n",
    "def select_country(state):\n",
    "    \"\"\"\n",
    "    helper function to handle\n",
    "    state changes and update\n",
    "    \"\"\"\n",
    "    \n",
    "    # update widgets\n",
    "    new_i = widgets.interactive(filter_and_graph, state=stateW, county=df3.query(f'state==\"{state[\"new\"]}\"').county.unique())\n",
    "    i.children = new_i.children\n",
    "\n",
    "# initial dropdown options\n",
    "stateW = widgets.Dropdown(options=df3.state.unique(), description='state: ')\n",
    "init = stateW.value\n",
    "countyW = df3.query(f'state==\"{init}\"').county.unique()\n",
    "\n",
    "# observe selection changes from dropdown and caller helper function\n",
    "# to update second dropdown list\n",
    "stateW.observe(select_country, 'value')\n",
    "\n",
    "i = widgets.interactive(filter_and_graph, state=stateW, county=countyW)\n",
    "\n",
    "display(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox_2",
   "language": "python",
   "name": "sandbox_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
